{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "744798 total words, 72633 unique words\n",
      "keep the top 72633 words\n"
     ]
    }
   ],
   "source": [
    "from reader import create_vocab\n",
    "vocab = create_vocab(100, 72633,'de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('64bit', '')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import platform \n",
    "platform.architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## w2v fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-26 22:10:33,358 INFO loading projection weights from /sharedfolder/w2v/german/dewiki_20180420_300d.txt.bz2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ccfd62508825>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/sharedfolder/w2v/german/dewiki_20180420_300d.txt.bz2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1496\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1497\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1498\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline_no\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unexpected end of input; is count incorrect or file otherwise damaged?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/bz2.py\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_can_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/_compression.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbyte_view\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_view\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mbyte_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/_compression.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                     \u001b[0mrawblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrawblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('/sharedfolder/w2v/german/dewiki_20180420_300d.txt.bz2', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19607\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "for v in vocab:\n",
    "    if v in model.wv.vocab:\n",
    "        c+=1\n",
    "print(c)\n",
    "#print('hit:{}'.format(c/15003))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-25 08:48:52,476 INFO saving Word2VecKeyedVectors object under /sharedfolder/w2v/german/w2v_emb, separately None\n",
      "2019-08-25 08:48:52,478 INFO not storing attribute vectors_norm\n",
      "2019-08-25 08:48:52,480 INFO storing np array 'vectors' to /sharedfolder/w2v/german/w2v_emb.vectors.npy\n",
      "2019-08-25 08:49:51,360 INFO saved /sharedfolder/w2v/german/w2v_emb\n"
     ]
    }
   ],
   "source": [
    "model.save('/sharedfolder/w2v/german/w2v_emb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-27 17:23:08,328 INFO Loading pre-train embeddings from: /sharedfolder/w2v/german/w2v_emb\n",
      "2019-08-27 17:23:08,330 INFO loading Word2VecKeyedVectors object from /sharedfolder/w2v/german/w2v_emb\n",
      "2019-08-27 17:23:15,978 INFO loading vectors from /sharedfolder/w2v/german/w2v_emb.vectors.npy with mmap=None\n",
      "2019-08-27 17:23:17,529 INFO setting ignored attribute vectors_norm to None\n",
      "2019-08-27 17:23:17,536 INFO loaded /sharedfolder/w2v/german/w2v_emb\n",
      "2019-08-27 17:23:17,541 INFO Loading trained embeddings from: ../preprocessed_data/german/w2v/full_trained/w2v_embedding_300\n",
      "2019-08-27 17:23:17,545 INFO loading Word2VecKeyedVectors object from ../preprocessed_data/german/w2v/full_trained/w2v_embedding_300\n",
      "2019-08-27 17:23:17,826 INFO loading wv recursively from ../preprocessed_data/german/w2v/full_trained/w2v_embedding_300.wv.* with mmap=None\n",
      "2019-08-27 17:23:17,827 INFO setting ignored attribute vectors_norm to None\n",
      "2019-08-27 17:23:17,828 INFO loading trainables recursively from ../preprocessed_data/german/w2v/full_trained/w2v_embedding_300.trainables.* with mmap=None\n",
      "2019-08-27 17:23:17,829 INFO loading vocabulary recursively from ../preprocessed_data/german/w2v/full_trained/w2v_embedding_300.vocabulary.* with mmap=None\n",
      "2019-08-27 17:23:17,829 INFO setting ignored attribute cum_table to None\n",
      "2019-08-27 17:23:17,830 INFO loaded ../preprocessed_data/german/w2v/full_trained/w2v_embedding_300\n",
      "2019-08-27 17:23:18,180 INFO hit: 0.45467327758454823\n",
      "2019-08-27 17:23:18,593 INFO disparity: 1.0\n",
      "2019-08-27 17:23:18,729 INFO #vectors: 6144, #dimensions: 300\n"
     ]
    }
   ],
   "source": [
    "from word_embed_reader import FineTuneEmbed_ortho_procrustes\n",
    "ft_emb = FineTuneEmbed_ortho_procrustes('/sharedfolder/w2v/german','w2v_emb','../preprocessed_data/german/w2v/full_trained', 'w2v_embedding_300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-11 12:16:16,879 INFO Loading pre-train embeddings from: ../preprocessed_data/w2v/fine_tuned/w2v_emb_1000k\n",
      "2019-08-11 12:16:16,880 INFO loading Word2VecKeyedVectors object from ../preprocessed_data/w2v/fine_tuned/w2v_emb_1000k\n",
      "2019-08-11 12:16:19,477 INFO loading vectors from ../preprocessed_data/w2v/fine_tuned/w2v_emb_1000k.vectors.npy with mmap=None\n",
      "2019-08-11 12:16:19,960 INFO setting ignored attribute vectors_norm to None\n",
      "2019-08-11 12:16:19,964 INFO loaded ../preprocessed_data/w2v/fine_tuned/w2v_emb_1000k\n",
      "2019-08-11 12:16:19,965 INFO Loading trained embeddings from: ../preprocessed_data/w2v/full_trained/w2v_embedding_300\n",
      "2019-08-11 12:16:19,966 INFO loading Word2VecKeyedVectors object from ../preprocessed_data/w2v/full_trained/w2v_embedding_300\n",
      "2019-08-11 12:16:20,186 INFO loading wv recursively from ../preprocessed_data/w2v/full_trained/w2v_embedding_300.wv.* with mmap=None\n",
      "2019-08-11 12:16:20,187 INFO setting ignored attribute vectors_norm to None\n",
      "2019-08-11 12:16:20,188 INFO loading trainables recursively from ../preprocessed_data/w2v/full_trained/w2v_embedding_300.trainables.* with mmap=None\n",
      "2019-08-11 12:16:20,188 INFO loading vocabulary recursively from ../preprocessed_data/w2v/full_trained/w2v_embedding_300.vocabulary.* with mmap=None\n",
      "2019-08-11 12:16:20,189 INFO setting ignored attribute cum_table to None\n",
      "2019-08-11 12:16:20,190 INFO loaded ../preprocessed_data/w2v/full_trained/w2v_embedding_300\n",
      "2019-08-11 12:16:20,850 INFO hit: 0.945600972496581\n",
      "2019-08-11 12:16:21,420 INFO KCCA fit started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CCA, gaussian kernel, regularization = 0.0100, 300 components\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-11 13:05:30,527 INFO KCCA transformation complete\n",
      "2019-08-11 13:05:30,887 INFO #vectors: 12446, #dimensions: 300\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from word_embed_reader import FineTuneEmbed_kcca\n",
    "ft_emb = FineTuneEmbed_kcca('../preprocessed_data/w2v/fine_tuned','w2v_emb_1000k','../preprocessed_data/w2v/full_trained', 'w2v_embedding_300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word_embed_reader import FineTuneEmbed_ortho_procrustes\n",
    "ft_emb = FineTuneEmbed_ortho_procrustes('../preprocessed_data/w2v/fine_tuned','w2v_emb_1000k','../preprocessed_data/w2v/full_trained', 'w2v_embedding_300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast-text fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import gensim\n",
    "model=gensim.models.KeyedVectors.load_word2vec_format('/sharedfolder/fasttext/german/cc.de.300.vec.gz', binary=False, limit=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('/sharedfolder/fasttext/german/wiki.de.vec', binary=False, limit=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/sharedfolder/fasttext/german/fasttext_pre_trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-23 16:36:24,406 INFO loading Word2VecKeyedVectors object from ../preprocessed_data/german/fasttext/full_trained/w2v_embedding_skipgram_300\n",
      "2019-08-23 16:36:24,747 INFO loading wv recursively from ../preprocessed_data/german/fasttext/full_trained/w2v_embedding_skipgram_300.wv.* with mmap=None\n",
      "2019-08-23 16:36:24,748 INFO loading vectors_ngrams from ../preprocessed_data/german/fasttext/full_trained/w2v_embedding_skipgram_300.wv.vectors_ngrams.npy with mmap=None\n",
      "2019-08-23 16:36:25,619 INFO setting ignored attribute vectors_vocab_norm to None\n",
      "2019-08-23 16:36:25,620 INFO setting ignored attribute vectors_ngrams_norm to None\n",
      "2019-08-23 16:36:25,621 INFO setting ignored attribute buckets_word to None\n",
      "2019-08-23 16:36:25,622 INFO setting ignored attribute vectors_norm to None\n",
      "2019-08-23 16:36:25,622 INFO loading trainables recursively from ../preprocessed_data/german/fasttext/full_trained/w2v_embedding_skipgram_300.trainables.* with mmap=None\n",
      "2019-08-23 16:36:25,623 INFO loading vectors_ngrams_lockf from ../preprocessed_data/german/fasttext/full_trained/w2v_embedding_skipgram_300.trainables.vectors_ngrams_lockf.npy with mmap=None\n",
      "2019-08-23 16:36:26,453 INFO loading vocabulary recursively from ../preprocessed_data/german/fasttext/full_trained/w2v_embedding_skipgram_300.vocabulary.* with mmap=None\n",
      "2019-08-23 16:36:26,453 INFO loaded ../preprocessed_data/german/fasttext/full_trained/w2v_embedding_skipgram_300\n"
     ]
    }
   ],
   "source": [
    "model1=gensim.models.KeyedVectors.load('../preprocessed_data/german/fasttext/full_trained/w2v_embedding_skipgram_300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13513"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model1.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-26 22:05:21,042 INFO Loading hybrid embeddings for fine tuning from: /sharedfolder/fasttext/german/hybrid_embed_ortho_procrsutes\n",
      "2019-08-26 22:05:21,194 INFO #vectors: 11747, #dimensions: 300\n"
     ]
    }
   ],
   "source": [
    "from word_embed_reader import FineTuneEmbed_ortho_procrustes\n",
    "ft_emb = FineTuneEmbed_ortho_procrustes('/sharedfolder/fasttext/german','fasttext_pre_trained','../preprocessed_data/german/fasttext/full_trained', 'w2v_embedding_skipgram_300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-24 10:08:03,407 INFO Loading pre-train embeddings from: /sharedfolder/fasttext/german/fasttext_pre_trained\n",
      "2019-08-24 10:08:03,409 INFO loading Word2VecKeyedVectors object from /sharedfolder/fasttext/german/fasttext_pre_trained\n",
      "2019-08-24 10:08:05,603 INFO loading vectors from /sharedfolder/fasttext/german/fasttext_pre_trained.vectors.npy with mmap=None\n",
      "2019-08-24 10:08:06,042 INFO setting ignored attribute vectors_norm to None\n",
      "2019-08-24 10:08:06,047 INFO loaded /sharedfolder/fasttext/german/fasttext_pre_trained\n",
      "2019-08-24 10:08:06,050 INFO Loading trained embeddings from: ../preprocessed_data/german/fasttext/full_trained/w2v_embedding_skipgram_300\n",
      "2019-08-24 10:08:06,054 INFO loading Word2VecKeyedVectors object from ../preprocessed_data/german/fasttext/full_trained/w2v_embedding_skipgram_300\n",
      "2019-08-24 10:08:06,373 INFO loading wv recursively from ../preprocessed_data/german/fasttext/full_trained/w2v_embedding_skipgram_300.wv.* with mmap=None\n",
      "2019-08-24 10:08:06,374 INFO loading vectors_ngrams from ../preprocessed_data/german/fasttext/full_trained/w2v_embedding_skipgram_300.wv.vectors_ngrams.npy with mmap=None\n",
      "2019-08-24 10:08:07,204 INFO setting ignored attribute vectors_vocab_norm to None\n",
      "2019-08-24 10:08:07,205 INFO setting ignored attribute vectors_ngrams_norm to None\n",
      "2019-08-24 10:08:07,206 INFO setting ignored attribute buckets_word to None\n",
      "2019-08-24 10:08:07,206 INFO setting ignored attribute vectors_norm to None\n",
      "2019-08-24 10:08:07,207 INFO loading trainables recursively from ../preprocessed_data/german/fasttext/full_trained/w2v_embedding_skipgram_300.trainables.* with mmap=None\n",
      "2019-08-24 10:08:07,208 INFO loading vectors_ngrams_lockf from ../preprocessed_data/german/fasttext/full_trained/w2v_embedding_skipgram_300.trainables.vectors_ngrams_lockf.npy with mmap=None\n",
      "2019-08-24 10:08:08,030 INFO loading vocabulary recursively from ../preprocessed_data/german/fasttext/full_trained/w2v_embedding_skipgram_300.vocabulary.* with mmap=None\n",
      "2019-08-24 10:08:08,031 INFO loaded ../preprocessed_data/german/fasttext/full_trained/w2v_embedding_skipgram_300\n",
      "2019-08-24 10:08:08,656 INFO hit: 0.8693110338192851\n",
      "2019-08-24 10:08:09,180 INFO CCA fit started\n",
      "2019-08-24 10:16:02,202 INFO CCA transformation complete\n",
      "2019-08-24 10:16:02,614 INFO #vectors: 11747, #dimensions: 300\n"
     ]
    }
   ],
   "source": [
    "from word_embed_reader import FineTuneEmbed_cca\n",
    "ft_emb = FineTuneEmbed_cca('/sharedfolder/fasttext/german','fasttext_pre_trained','../preprocessed_data/german/fasttext/full_trained', 'w2v_embedding_skipgram_300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-15 21:59:43,046 INFO Loading pre-train embeddings from: ../preprocessed_data/fasttext/fine_tuned/fasttext_pre_trained\n",
      "2019-08-15 21:59:43,049 INFO loading Word2VecKeyedVectors object from ../preprocessed_data/fasttext/fine_tuned/fasttext_pre_trained\n",
      "2019-08-15 21:59:48,249 INFO loading vectors from ../preprocessed_data/fasttext/fine_tuned/fasttext_pre_trained.vectors.npy with mmap=None\n",
      "2019-08-15 22:00:46,745 INFO setting ignored attribute vectors_norm to None\n",
      "2019-08-15 22:00:46,752 INFO loaded ../preprocessed_data/fasttext/fine_tuned/fasttext_pre_trained\n",
      "2019-08-15 22:00:46,757 INFO Loading trained embeddings from: ../preprocessed_data/fasttext/full_trained/w2v_embedding_skipgram_300\n",
      "2019-08-15 22:00:46,758 INFO loading Word2VecKeyedVectors object from ../preprocessed_data/fasttext/full_trained/w2v_embedding_skipgram_300\n",
      "2019-08-15 22:00:55,784 INFO loading wv recursively from ../preprocessed_data/fasttext/full_trained/w2v_embedding_skipgram_300.wv.* with mmap=None\n",
      "2019-08-15 22:00:55,786 INFO loading vectors_ngrams from ../preprocessed_data/fasttext/full_trained/w2v_embedding_skipgram_300.wv.vectors_ngrams.npy with mmap=None\n",
      "2019-08-15 22:01:01,425 INFO setting ignored attribute vectors_norm to None\n",
      "2019-08-15 22:01:01,426 INFO setting ignored attribute vectors_vocab_norm to None\n",
      "2019-08-15 22:01:01,427 INFO setting ignored attribute vectors_ngrams_norm to None\n",
      "2019-08-15 22:01:01,428 INFO setting ignored attribute buckets_word to None\n",
      "2019-08-15 22:01:01,429 INFO loading vocabulary recursively from ../preprocessed_data/fasttext/full_trained/w2v_embedding_skipgram_300.vocabulary.* with mmap=None\n",
      "2019-08-15 22:01:01,431 INFO loading trainables recursively from ../preprocessed_data/fasttext/full_trained/w2v_embedding_skipgram_300.trainables.* with mmap=None\n",
      "2019-08-15 22:01:01,432 INFO loading vectors_ngrams_lockf from ../preprocessed_data/fasttext/full_trained/w2v_embedding_skipgram_300.trainables.vectors_ngrams_lockf.npy with mmap=None\n",
      "2019-08-15 22:01:09,066 INFO loaded ../preprocessed_data/fasttext/full_trained/w2v_embedding_skipgram_300\n",
      "2019-08-15 22:01:10,216 INFO hit: 0.9358552631578947\n",
      "2019-08-15 22:01:11,132 INFO KCCA fit started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CCA, gaussian kernel, regularization = 0.0100, 300 components\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b8a45b5c83b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mword_embed_reader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFineTuneEmbed_kcca\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mft_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFineTuneEmbed_kcca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../preprocessed_data/fasttext/fine_tuned'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'fasttext_pre_trained'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'../preprocessed_data/fasttext/full_trained'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w2v_embedding_skipgram_300'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/sharedfolder/master-thesis/code/word_embed_reader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, pre_train_data_path, pre_train_emb_name, full_train_data_path, full_train_emb_name)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mcca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrccaMod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumCC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernelcca\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mktype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gaussian\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"KCCA fit started\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mcancomps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcommon_pre_train_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommon_full_train_embedding\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcommon_pre_train_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancomps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0mcommon_full_train_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancomps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sharedfolder/master-thesis/code/rccaMod.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCCA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sharedfolder/master-thesis/code/rccaMod.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training CCA, regularization = %0.4f, %d components\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumCC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mcomps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkcca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumCC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernelcca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernelcca\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mktype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgausigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgausigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancorrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernelcca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernelcca\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sharedfolder/master-thesis/code/rccaMod.py\u001b[0m in \u001b[0;36mkcca\u001b[0;34m(data, reg, numCC, kernelcca, ktype, gausigma, degree)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;31m# Get the kernel auto- and cross-covariance matrices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkernelcca\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mcrosscovs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mki\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mki\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkernel\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mcrosscovs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mki\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mki\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkernel\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sharedfolder/master-thesis/code/rccaMod.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;31m# Get the kernel auto- and cross-covariance matrices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkernelcca\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mcrosscovs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mki\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mki\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkernel\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mcrosscovs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mki\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mki\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkernel\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from word_embed_reader import FineTuneEmbed_kcca\n",
    "ft_emb = FineTuneEmbed_kcca('../preprocessed_data/fasttext/fine_tuned','fasttext_pre_trained','../preprocessed_data/fasttext/full_trained', 'w2v_embedding_skipgram_300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FineTuneEmbed_ortho_procrustes' object has no attribute 'most_similar'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2ac373c2e264>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mft_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'teacher'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'FineTuneEmbed_ortho_procrustes' object has no attribute 'most_similar'"
     ]
    }
   ],
   "source": [
    "ft_emb.most_similar('teacher')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit:0.9961175818080976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "for v in vocab:\n",
    "    if v in model.wv.vocab:\n",
    "        c+=1\n",
    "print('hit:{}'.format(c/len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../preprocessed_data/fasttext/fine_tuned/fasttext_pre_trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = gensim.models.KeyedVectors.load('../preprocessed_data/fasttext/fine_tuned/fasttext_pre_trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "999994"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(m.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## German w2v pretrained fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import tempfile\n",
    "import os\n",
    "import shutil\n",
    "temp = tempfile.TemporaryFile(mode='w+t')\n",
    "try:\n",
    "    fin=codecs.open('/sharedfolder/w2v/german/vectors.txt','r',encoding='utf-8')\n",
    "    fout = codecs.open('/sharedfolder/w2v/german/vectors_modified.txt','w',encoding='utf-8')\n",
    "    total_vocab = 0\n",
    "    vector_len = 0\n",
    "    for line in fin:\n",
    "        line=line.strip()\n",
    "        word, vector=line.split()[0][2:-1],line.split()[1:]\n",
    "        total_vocab = total_vocab+1\n",
    "        vector_len = len(vector)\n",
    "        temp.write(word+' '+\" \".join(vector)+'\\n')\n",
    "    fout.write(str(total_vocab)+' '+str(vector_len)+'\\n')\n",
    "    temp.seek(0)\n",
    "    shutil.copyfileobj(temp, fout)\n",
    "    fout.close()\n",
    "    fin.close()\n",
    "finally:\n",
    "    temp.close()\n",
    "    #os.remove(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
